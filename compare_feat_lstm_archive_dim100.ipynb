{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c480245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19326681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_stat(x):\n",
    "#     ipdb.set_trace()\n",
    "#     return f'{x.mean():.2f}$\\pm${x.std():.2f}'\n",
    "    return f'{x.mean():.2f} ({x.quantile(.025):.2f}-{x.quantile(.975):.2f})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074915fc",
   "metadata": {},
   "source": [
    "# bootstrap metric fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd492d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from pqdm.processes import pqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def ballogloss(y_true, y_pred):\n",
    "    weights = np.array([1/len(y_true[y_true==i]) for i in y_true])\n",
    "    weights /= np.sum(weights)\n",
    "    return log_loss(y_true, y_pred, sample_weight=weights)\n",
    "\n",
    "def bootstrap_metric(fn, n, y_true, y_pred, n_samples):\n",
    "    ys_true, ys_pred = resample(y_true, y_pred, n_samples=n_samples)\n",
    "    return {\n",
    "        'metric':fn.__name__,\n",
    "        'value':fn(ys_true,ys_pred),\n",
    "        'bootstrap':n\n",
    "    }\n",
    "        \n",
    "def bootstrap_metrics(y_true, y_pred, n_bootstraps=100, n_samples=None):\n",
    "    # This line is the strange hack https://github.com/tqdm/tqdm/issues/485\n",
    "    print(' ', end='', flush=True)\n",
    "    metrics = [roc_auc_score, average_precision_score, log_loss, ballogloss]\n",
    "    scores = []\n",
    "    for n in tqdm(range(n_bootstraps)):\n",
    "        for fn in metrics:\n",
    "            scores.append(bootstrap_metric(fn,n,y_true,y_pred,n_samples))\n",
    "    return scores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0642573",
   "metadata": {},
   "source": [
    "# load lstm results\n",
    "- to save time, this step is saved to a csv and re-loaded below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0326ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mimic3models.common_utils import phenotype_names \n",
    "# from tqdm.notebook import tqdm\n",
    "# df_lstm = pd.read_csv('results_lstm/k_lstm.n256.d0.3.dep1.bs8.ts1.0.epoch100.test0.4631129801273346.state.csv')\n",
    "df_lstm = pd.read_csv('results_lstm/k_lstm.n256.d0.3.dep1.bs8.ts1.0.epoch14.test0.4256526231765747.state.csv')\n",
    "\n",
    "frames = []\n",
    "n_bootstraps = 100\n",
    "    \n",
    "def bootstrap_phenotype(i,p):\n",
    "    # This line is the strange hack https://github.com/tqdm/tqdm/issues/485\n",
    "    print(' ', end='', flush=True)\n",
    "    y_true = df_lstm[f'label_{i+1}'].values\n",
    "    y_pred = df_lstm[f'pred_{i+1}'].values\n",
    "    scores = bootstrap_metrics(y_true, y_pred)\n",
    "    results = []\n",
    "    for s in scores:\n",
    "        result = {\n",
    "            'method':'LSTM',\n",
    "            'task':p.replace(';','')\n",
    "        }\n",
    "        result.update(s)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "frames = pqdm([(i,p) for i,p in enumerate(phenotype_names)],\n",
    "              bootstrap_phenotype,\n",
    "              n_jobs=20,\n",
    "              argument_type='args'\n",
    "             )\n",
    "\n",
    "flat_frames = []\n",
    "for f in frames:\n",
    "    flat_frames.extend(f)\n",
    "df_lsr = pd.DataFrame.from_records(flat_frames)\n",
    "df_lsr.loc[:,'data'] = 'raw'\n",
    "df_lsr.loc[:,'n_nodes'] = 341249\n",
    "# Number of nodes gotten with this code:\n",
    "# from mimic3models.keras_models.lstm import Network\n",
    "# lstm = Network(dim=256, batch_norm=False, dropout=0.3, rec_dropout=0.1, task='ph')\n",
    "# lstm.summary()\n",
    "# Model: \"network\"\n",
    "# _________________________________________________________________\n",
    "#  Layer (type)                Output Shape              Param #\n",
    "# =================================================================\n",
    "#  X (InputLayer)              [(None, None, 76)]        0\n",
    "\n",
    "#  masking (Masking)           (None, None, 76)          0\n",
    "\n",
    "#  lstm (LSTM)                 (None, 256)               340992\n",
    "\n",
    "#  dropout (Dropout)           (None, 256)               0\n",
    "\n",
    "#  dense (Dense)               (None, 1)                 257\n",
    "\n",
    "# =================================================================\n",
    "# Total params: 341,249\n",
    "# Trainable params: 341,249\n",
    "# Non-trainable params: 0\n",
    "    \n",
    "df_lsr\n",
    "df_lsr.to_csv('lstm_bootstrapped.k_lstm.n256.d0.3.dep1.bs8.ts1.0.epoch14.test0.4256526231765747.state.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a121b6cf",
   "metadata": {},
   "source": [
    "## Get Micro-averaged scores for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd04052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YTrues = []\n",
    "# YPreds = []\n",
    "# for i in range(len(phenotype_names)):\n",
    "#     y_true = df_lstm[f'label_{i+1}'].values\n",
    "#     y_pred = df_lstm[f'pred_{i+1}'].values\n",
    "#     YTrues.extend(y_true)\n",
    "#     YPreds.extend(y_pred)\n",
    "# len(YTrues)==len(YPreds)\n",
    "\n",
    "# results = bootstrap_metrics(np.asarray(YTrues), np.asarray(YPreds), n_samples=10000)\n",
    "# # micro_lstm = pd.DataFrame.from_records(frames,columns=frames[0].keys())\n",
    "# # micro_lstm\n",
    "# micro_lstm = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39d612",
   "metadata": {},
   "source": [
    "# FEAT and LR results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec088d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdirs = [\n",
    "         'results/lpc/results_linear/',\n",
    "         'results/lpc/results_archive_22-06-07/',\n",
    "         'results/lpc/results_archive_dim100_22-06-07/',\n",
    "#          'results/lpc/results_feat_22-05-26/',\n",
    "        ]\n",
    "# rdirs = ['arch_test/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99117f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rframes = []\n",
    "dropcols = ['metrics','acc', 'prec0', 'prec1', 'rec0', 'rec1', 'minpse' ]\n",
    "\n",
    "metrics = ['roc_auc_score','average_precision_score','ballogloss','log_loss']\n",
    "globs = []\n",
    "for r in rdirs:\n",
    "    globs.extend(glob(r+'/results/*.csv'))\n",
    "for f in globs:\n",
    "#     print(f)\n",
    "    d = pd.read_csv(f)\n",
    "    if 'metrics' in d.columns:\n",
    "        d = d.drop('metrics',axis=1)\n",
    "    if '.feat.' in f:\n",
    "        d['method'] = 'FEAT'\n",
    "        if 'dim100_' in f:\n",
    "            d['method'] = d['method']+'-100'\n",
    "    rframes.append(d)\n",
    "    \n",
    "print('loaded',len(rframes),'frames')\n",
    "df_r = pd.concat(rframes) #.dropna()\n",
    "\n",
    "# rename metrics\n",
    "metnames = {'auroc':'roc_auc_score',\n",
    "            'auprc':'average_precision_score',\n",
    "            'logloss':'log_loss'\n",
    "           }\n",
    "df_r['metric'] = df_r['metric'].apply(lambda x: metnames[x] if x in metnames.keys() else x)\n",
    "df_r['task'] = df_r['task'].apply(lambda x: x.replace('-',' '))\n",
    "df_r = df_r.loc[df_r.metric.isin(metrics)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r.method.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcf8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r.loc[(df_r.method=='LR')\n",
    "         & (df_r.task.str.contains('Septicemia'))\n",
    "#          & (df_r.metric=='log_loss')\n",
    "#          & (df_r.fold=='val')\n",
    "         & (df_r.run_id=='3194bf64dd0e11ec9865a0369feec84c')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lr_pred_name = \"results/lpc/results_linear/predictions/lr.run_3194bf64dd0e11ec9865a0369feec84c.param_4.json\"\n",
    "lr_res_name = 'results/lpc/results_linear/results/Septicemia-(except-in-labor).lr.run_3194bf64dd0e11ec9865a0369feec84c.param_4.csv'\n",
    "lr_params = 'results/lpc/results_linear/results/Septicemia-(except-in-labor).lr.run_3194bf64dd0e11ec9865a0369feec84c.param_4.params'\n",
    "df_lr_sept = pd.read_csv(lr_res_name)\n",
    "with open(lr_pred_name,'r') as file:\n",
    "    d = json.load(file)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lr_params,'r') as file:\n",
    "    p = json.load(file)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c03a7",
   "metadata": {},
   "source": [
    "# parameter selection: pick results with best val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb\n",
    "def not_overfit_best(data, metric='roc_auc_score', threshold=0.05):\n",
    "    indexer = ['run_id','task','method','archive_id']\n",
    "    dfp = data.pivot(\n",
    "             index = indexer,\n",
    "             columns=['fold','metric'],\n",
    "             values='value'\n",
    "            )\n",
    "    overfitting = (dfp[('train',metric)] - dfp[('val',metric)])/dfp[('train',metric)]\n",
    "    \n",
    "    mask =  overfitting <= threshold\n",
    "    while np.sum(mask) == 0:\n",
    "        threshold += 0.01\n",
    "        mask =  overfitting <= threshold\n",
    "    print(data.iloc[0]['method'],data.iloc[0]['task'],'mask sum:',np.sum(mask))     \n",
    "    dfp = dfp[mask].nlargest(1,columns=[('val',metric)])\n",
    "    try:\n",
    "        idx = dfp.reset_index().melt(id_vars=indexer).iloc[0][indexer].to_dict() \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ipdb.set_trace()\n",
    "#     print(idx)\n",
    "    return idx\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8487e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model_selection import (smallest_of_best_three_quartiles,\n",
    "                             best_of_smallest_three_quartiles, \n",
    "                             best, \n",
    "                             smallest_of_best_quartile)\n",
    "selector = best_of_smallest_three_quartiles\n",
    "# selector = smallest_of_best_three_quartiles\n",
    "# selector = smallest_of_best_quartile\n",
    "bests = []\n",
    "metric={}\n",
    "metric['FEAT'] = 'roc_auc_score'\n",
    "metric['FEAT-100'] = 'roc_auc_score'\n",
    "metric['LR'] = 'log_loss'\n",
    "metric['LR-10'] = 'log_loss'\n",
    "metric['LR-100'] = 'log_loss'\n",
    "\n",
    "lr_indexer = ['run_id','task','method']\n",
    "feat_indexer = ['run_id','task','method','archive_id']\n",
    "\n",
    "for (method,task),dfg in df_r.groupby(['method','task']):\n",
    "    if 'LR' in method:\n",
    "        dfg = dfg.loc[(dfg.fold=='val') & (dfg.metric==metric[method])]\n",
    "        tmp = dfg.nsmallest(1, columns='value') \n",
    "        idx = tmp[lr_indexer].to_dict(orient='records')[0]\n",
    "    elif method=='FEAT-100':\n",
    "#         tmp = best_of_smallest_three_quartiles(dfg, metric='value', size='n_nodes')\n",
    "#         idx = tmp.reset_index().melt(id_vars=feat_indexer).iloc[0][feat_indexer].to_dict() \n",
    "        idx = not_overfit_best(dfg, metric=metric[method], threshold=0.2)\n",
    "    else:\n",
    "        idx = not_overfit_best(dfg, metric=metric[method], threshold=0.2)\n",
    "        \n",
    "    bests.append(idx)\n",
    "        \n",
    "    \n",
    "\n",
    "dfs = []\n",
    "\n",
    "for elem in bests:\n",
    "    df = df_r\n",
    "    for k,v in elem.items():\n",
    "        df = df.loc[df[k]==v]\n",
    "    dfs.append(df)\n",
    "\n",
    "df_best = pd.concat(dfs)\n",
    "df_best = df_best.loc[df_best.fold=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b2003",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.method.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb\n",
    "# # Option: try heuristic from before (smallest of best 3 quartiles etc)\n",
    "# # val_metric = 'roc_auc_score'\n",
    "# # val_metric = 'average_precision_score'\n",
    "# val_metric = 'log_loss'\n",
    "# fn = np.min\n",
    "# dfs = []\n",
    "# for method, val_metric, fn in [\n",
    "#     ('FEAT','roc_auc_score',np.max),\n",
    "#     ('FEAT-100','roc_auc_score',np.max),\n",
    "#     ('LR','log_loss',np.min)\n",
    "#     ]:\n",
    "#     task_best = (df_r.loc[(df_r.method==method) & (df_r.fold=='val') & (df_r.metric==val_metric)]\n",
    "#                  .groupby(['method','task'],as_index=False)\n",
    "#                  ['value']\n",
    "#                  .apply(fn)\n",
    "#                  .reset_index()\n",
    "#                 )\n",
    "#     df = pd.merge(df_r,task_best,on=['method','task'],suffixes=('','_best')).set_index(['run_id','task','method','archive_id'])\n",
    "#     idx = df.loc[(df.fold=='val') & (df.metric==val_metric) & (df.value==df.value_best)].index.values\n",
    "#     df_best = df.loc[idx].reset_index()\n",
    "# #     pdb.set_trace()\n",
    "#     df_best = df_best.loc[df_best.fold=='test']\n",
    "#     dfs.append(df_best)\n",
    "# df_best = pd.concat(dfs)\n",
    "# df_best = df_best.drop_duplicates(subset=['task',\n",
    "#                                 'data',\n",
    "#                                 'param_id',\n",
    "#                                 'fold','model','n_nodes','metric','value','method','value_best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.loc[df_best['metric']=='roc_auc_score'].groupby(['task','metric','method'])['value'].mean().unstack().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126774fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.groupby('method').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba3494",
   "metadata": {},
   "source": [
    "## load feat predictions and calculate bootstrapped metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipdb \n",
    "\n",
    "def get_pred_name(x):\n",
    "    if  'lr' in x.method.lower():\n",
    "        return f\"{x.method.lower().replace('-','')}.run_{x.run_id}.param_{x.param_id}.json\" \n",
    "    else:\n",
    "        mthd = x.method.lower().split('-')[0]\n",
    "        return f\"{x.task.replace(' ','-')}.{mthd}.run_{x.run_id}.param_{x.param_id}.arc{int(x.archive_id)}.json\"\n",
    "        \n",
    "        \n",
    "pred_names = (df_best.apply(lambda x: get_pred_name(x), axis=1)\n",
    "              .unique()\n",
    "             )\n",
    "pframes = []\n",
    "# for f in [rdir + 'predictions/'+pn for pn in pred_names]:\n",
    "# #     print(f)\n",
    "# #     pframes.append(d)\n",
    "# # print(pframes)\n",
    "# df_p = pd.DataFrame.from_records(pframes)\n",
    "# df_p\n",
    "\n",
    "frames = []\n",
    "n_bootstraps = 100\n",
    "def bootstrap_phenotype(f):\n",
    "    with open(f,'r') as file:\n",
    "        d = json.load(file)\n",
    "    y_true = np.array(d['label'])\n",
    "    y_pred = np.array(d['pred'])\n",
    "#     ipdb.set_trace()\n",
    "    scores = bootstrap_metrics(y_true, y_pred)\n",
    "    results = []\n",
    "    if 'feat' in f:\n",
    "        idx = 1\n",
    "    else:\n",
    "        idx = 0\n",
    "    method = f.split('/')[-1].split('.')[idx].upper()\n",
    "    if 'LR10' in method:\n",
    "        method=method.replace('10','-10')\n",
    "    if 'dim100_' in f:\n",
    "        method += '-100'\n",
    "    for s in scores:\n",
    "        \n",
    "        res = {\n",
    "            'method':method,\n",
    "            'task':d['task'].replace('-',' ')\n",
    "        }\n",
    "        res.update(s)\n",
    "        results.append(res)\n",
    "#     if any([np.isnan(s['value']) for s in scores]):\n",
    "#         print(results)\n",
    "#         ipd.set_trace()\n",
    "    return results\n",
    "\n",
    "# frames = []\n",
    "# for f in [rdir + 'predictions/'+pn for pn in pred_names]:\n",
    "#     frames.append(bootstrap_phenotype(f))\n",
    "pred_files = []\n",
    "for p in pred_names:\n",
    "    found = False\n",
    "    for r in rdirs:\n",
    "        f = r+ 'predictions/'+p\n",
    "        if os.path.exists(f):\n",
    "            pred_files.append(f)\n",
    "            found=True\n",
    "            continue\n",
    "    if not found:\n",
    "        raise ValueError(f\"uh oh spaghetti-ohs, {p} not found\")\n",
    "    \n",
    "# frames = [bootstrap_phenotype(p) for p in tqdm(pred_files)]\n",
    "frames = pqdm(pred_files,\n",
    "              bootstrap_phenotype,\n",
    "              n_jobs=20,\n",
    "             )\n",
    "flat_frames = []\n",
    "for f in frames:\n",
    "    flat_frames.extend(f)\n",
    "# print(frames)\n",
    "df_feat_lr = pd.DataFrame.from_records(flat_frames)\n",
    "df_feat_lr.loc[:,'data'] = 'tsfresh'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ec2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_lr.groupby(['task','metric','method'])['value'].apply(nice_stat).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046221a",
   "metadata": {},
   "source": [
    "# Get Micro-averaged scores for FEAT and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YTrues = {}\n",
    "# YPreds = {}\n",
    "# YTruesByTask = {}\n",
    "# YPredsByTask = {}\n",
    "# for f in pred_files:\n",
    "#     with open(f,'r') as file:\n",
    "#         d = json.load(file)\n",
    "# #     import ipdb\n",
    "# #     ipdb.set_trace()\n",
    "#     y_true = np.array(d['label'])\n",
    "#     y_pred = np.array(d['pred'])\n",
    "#     if 'feat' in f:\n",
    "#         idx = 1\n",
    "#     else:\n",
    "#         idx = 0\n",
    "#     method = f.split('/')[-1].split('.')[idx].upper()\n",
    "#     if 'dim100_' in f:\n",
    "#         method += '-100'\n",
    "#     if not (method in YTrues.keys()):\n",
    "#         YTrues[method] = []\n",
    "#         YPreds[method] = []\n",
    "#         YTruesByTask[method] = {} \n",
    "#         YPredsByTask[method] = {} \n",
    "#     YTrues[method].extend(y_true)\n",
    "#     YPreds[method].extend(y_pred)\n",
    "#     YTruesByTask[method][d['task']] = y_true\n",
    "#     YPredsByTask[method][d['task']] = y_pred\n",
    "    \n",
    "\n",
    "# frames = []\n",
    "# # scores = pqdm([dict(y_true=np.asarray(YTrues[m]),y_pred=np.asarray(YPreds[m])) for m in YTrues.keys()],\n",
    "# #               bootstrap_metrics,\n",
    "# #               n_jobs=3,\n",
    "# #               argument_type='kwargs' \n",
    "# # )\n",
    "# # for scrs,m in zip(scores,YTrues.keys()):\n",
    "# #     [s.update({'method':m}) for s in scrs]\n",
    "# #     frames.extend(scrs)\n",
    "    \n",
    "# for m in YTrues.keys():\n",
    "#     scores = bootstrap_metrics(np.asarray(YTrues[m]),np.asarray(YPreds[m]), n_samples=10_000, n_bootstraps=100)\n",
    "#     [s.update({'method':m}) for s in scores]\n",
    "#     frames.extend(scores)\n",
    "# micro_feat_lr = pd.DataFrame.from_records(frames)\n",
    "\n",
    "# frames=[]\n",
    "# for m,v in YTruesByTask.items():\n",
    "#     for t in v.keys():\n",
    "#         scores = bootstrap_metrics(np.asarray(YTruesByTask[m][t]),np.asarray(YPredsByTask[m][t]), \n",
    "#                                    n_samples=100000, n_bootstraps=1)\n",
    "#         [s.update({'method':m,'task':t}) for s in scores]\n",
    "#         frames.extend(scores)\n",
    "               \n",
    "# scores_feat_lr = pd.DataFrame.from_records(frames)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7baf36",
   "metadata": {},
   "source": [
    "## sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step = 6281\n",
    "# for i,(task,vals) in enumerate(YTruesByTask['FEAT'].items()):\n",
    "#     print(task,\n",
    "#           all(YTrues['FEAT'][i*step:i*step+step] == vals)\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step = 6281\n",
    "# for m,v in YPredsByTask.items():\n",
    "#     for i,t in enumerate(v.keys()):\n",
    "#         print(m,t,\n",
    "#               all(YPreds[m][i*step:i*step+step] == YPredsByTask[m][t])\n",
    "#              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa96c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_feat_lr.loc[scores_feat_lr['metric']=='roc_auc_score'].groupby(['task','method']).mean().unstack().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set_context('paper')\n",
    "# sns.set(font_scale=1.2)\n",
    "# sns.set_style('whitegrid')\n",
    "\n",
    "# df_plt = scores_feat_lr.loc[scores_feat_lr.metric.isin(['roc_auc_score','average_precision_score'])]\n",
    "\n",
    "# df_plt['nice-task'] = df_plt['task'].apply(lambda x: task_names[x] if x in task_names.keys() else x)\n",
    "# # task_order = df_plt[df_plt.metric=='roc_auc_score'].groupby('nice-task')['value'].mean().sort_values().index[::-1]\n",
    "# task_order = df_plt[df_plt.metric=='average_precision_score'].groupby('nice-task')['value'].mean().sort_values().index[::-1]\n",
    "# print('task_order:',task_order)\n",
    "# g = sns.catplot(\n",
    "#     kind='point',\n",
    "#     estimator=np.median,\n",
    "# #     kind='strip',\n",
    "# #     showfliers=False,\n",
    "# #     dodge=False,\n",
    "#     data=df_plt,\n",
    "#     x='value',\n",
    "# #     y='task',\n",
    "#     y='nice-task',\n",
    "#     hue='method',\n",
    "#     order=task_order,\n",
    "# #     hue_order=['FEAT','FEAT-100','LR-10','LR-100','LR'], #,'LSTM'],\n",
    "#     hue_order=['FEAT','LR-10'], #,'LSTM'],\n",
    "#     col='metric',\n",
    "#     ci='sd',\n",
    "#     join=False,\n",
    "#     height=6,\n",
    "#     aspect=1.2,\n",
    "#     sharex=False,\n",
    "# #     palette='Spectral',\n",
    "# )\n",
    "# g.set(ylabel='',xlabel='')\n",
    "# # plt.xlabel('AUROC')\n",
    "\n",
    "# for i,ax in enumerate(g.axes.flat):\n",
    "#     ax.grid(True,axis='y')\n",
    "#     ax.grid(False,axis='x')\n",
    "# #     ax.set_title('')\n",
    "# #     if i == 0:\n",
    "# #         ax.set_title('AUROC')\n",
    "# # #         ax.set_title('AUPRC')\n",
    "# # #     elif i == 1:\n",
    "# # #         ax.set_title('AUPRC')\n",
    "# #     elif i == 1:\n",
    "# # #     elif i == 2:\n",
    "# #         ax.set_xscale('log')\n",
    "# #         ax.set_title('Model Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edca406",
   "metadata": {},
   "source": [
    "# case counts per task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca186b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = []\n",
    "\n",
    "# YTBT = YTruesByTask['LR']\n",
    "# for t,v in YTBT.items():\n",
    "#     frames.append(\n",
    "#         {\n",
    "#             'task': t,\n",
    "#             'cases':np.sum(v==1),\n",
    "#             'controls':np.sum(v==0),\n",
    "#             'prevalence':np.sum(v==1)/len(v),\n",
    "#         }\n",
    "#     )\n",
    "# case_counts= pd.DataFrame.from_records(frames)\n",
    "# case_counts.sort_values(by='cases',ascending=False) #.value_counts() #['cases'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = scores_feat_lr.merge(case_counts,on='task')\n",
    "# metric='average_precision_score'\n",
    "# x = (tmp.loc[tmp.metric==metric]\n",
    "#  .groupby(['task','cases','method'])\n",
    "#  ['value']\n",
    "#  .mean()\n",
    "#  .unstack()\n",
    "#  .sort_values(by='cases',ascending=False)\n",
    "# )\n",
    "# x['$\\Delta$'] = x['FEAT']-x['LR']\n",
    "# x['$\\Delta$100'] = x['FEAT-100']-x['LR']\n",
    "# x.round(3)\n",
    "# # tmp.sort_values(by='cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = (tmp\n",
    "#      .groupby(['task','metric','bootstrap','method'])\n",
    "#      ['value']\n",
    "#      .mean()\n",
    "#      .unstack()\n",
    "# )\n",
    "# x['FEAT - LR'] = x['FEAT']-x['LR']\n",
    "# x['FEAT-100 - LR'] = x['FEAT-100']-x['LR']\n",
    "# x = \\\n",
    "# (x.reset_index()\n",
    "#  .melt(id_vars=['task','metric','bootstrap'],\n",
    "#        var_name='method',\n",
    "#        value_name='value'\n",
    "#       )\n",
    "# )\n",
    " \n",
    "# # tmp.merge(x.reset_index(), on = ['task','metric','bootstrap'])\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set_context('paper')\n",
    "# sns.set(font_scale=1.2)\n",
    "# sns.set_style('whitegrid')\n",
    "\n",
    "# df_plt = x.copy()\n",
    "# df_plt = df_plt.loc[df_plt.metric.isin(['roc_auc_score','average_precision_score'])]\n",
    "\n",
    "# df_plt['nice-task'] = df_plt['task'].apply(lambda x: task_names[x] if x in task_names.keys() else x)\n",
    "# task_order = df_plt[df_plt.metric=='average_precision_score'].groupby('nice-task')['value'].mean().sort_values().index[::-1]\n",
    "# print('task_order:',task_order)\n",
    "# g = sns.catplot(\n",
    "#     kind='point',\n",
    "#     estimator=np.median,\n",
    "#     data=df_plt,\n",
    "#     x='value',\n",
    "#     y='nice-task',\n",
    "#     hue='method',\n",
    "#     order=task_order,\n",
    "#     hue_order=['FEAT - LR', 'FEAT-100 - LR'],\n",
    "#     col='metric',\n",
    "#     ci='sd',\n",
    "#     join=False,\n",
    "#     height=6,\n",
    "#     aspect=1.2,\n",
    "#     sharex=False,\n",
    "#     palette='cividis_r',\n",
    "# )\n",
    "# g.set(ylabel='',xlabel='')\n",
    "# # plt.xlabel('AUROC')\n",
    "\n",
    "# for i,ax in enumerate(g.axes.flat):\n",
    "#     ax.grid(True,axis='y')\n",
    "#     ax.grid(False,axis='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795fee6a",
   "metadata": {},
   "source": [
    "### combine micro scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59459dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro_lstm['method']='LSTM'\n",
    "\n",
    "# micro_df = micro_feat_lr.append(micro_lstm)\n",
    "# micro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro_df.groupby('method')['value'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set_context('paper')\n",
    "# sns.set(font_scale=1.2)\n",
    "# sns.set_style('whitegrid')\n",
    "# # n_nodes = df_best.melt(id_vars=['run_id','method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "# # feat_lr = pd.concat((df_best,n_nodes))\n",
    "\n",
    "# # df_best_lsr = df_lsr.groupby(['method','task','metric','n_nodes'])['value'].mean().reset_index()\n",
    "# # n_nodes = df_best_lsr.melt(id_vars=['method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "# # lstm = pd.concat((df_best_lsr,n_nodes))\n",
    "\n",
    "# # df_plt = pd.concat((feat_lr,lstm))\n",
    "# df_plt = micro_df.copy()\n",
    "# df_plt = df_plt.loc[df_plt.metric.isin(['roc_auc_score','average_precision_score'])]\n",
    "# g = sns.catplot(\n",
    "#     kind='box',\n",
    "#     showfliers=False,\n",
    "#     dodge=False,\n",
    "#     data=df_plt,\n",
    "#     y='value',\n",
    "#     x='method',\n",
    "#     col='metric',\n",
    "#     sharex=False,\n",
    "#     legend=False,\n",
    "#     color='w',\n",
    "# )\n",
    "\n",
    "# for i,ax in enumerate(g.axes.flat):\n",
    "#     ax.grid(True,axis='y')\n",
    "#     ax.grid(False,axis='x')\n",
    "#     ttl = ax.get_title()[8:]\n",
    "# #     if len(ttl) > 40:\n",
    "# #         words = ttl.split(' ')\n",
    "# #         i = math.floor(len(words)/2)\n",
    "# #         ttl = (' '.join(words[:i])\n",
    "# #                +'\\n'\n",
    "# #                +' '.join(words[i:])\n",
    "# #               )\n",
    "#     ax.set_xlabel(ttl.replace('_',' '))\n",
    "#     ax.set_title('')\n",
    "#     if i == 0:\n",
    "# #         ax.set_xlabel('Micro AUROC')\n",
    "#         ax.set_title('Micro AUROC')\n",
    "#     elif i == 1:\n",
    "# #         ax.set_xlabel('Micro AUPRC')\n",
    "#         ax.set_title('Micro AUPRC')\n",
    "#     for j, child in enumerate(ax._children):\n",
    "#         if hasattr(child,'_edgecolor'):\n",
    "#             child.set_edgecolor('black')\n",
    "#     for j, line in enumerate(ax.lines):\n",
    "#         line.set_color('k')\n",
    "# #         # iterate over whiskers and median lines\n",
    "# # #         for k in range(6*j,6*(j+1)):\n",
    "# # #          box.lines[k].set_color('black') \n",
    "# #         box.set_color('black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb03944",
   "metadata": {},
   "source": [
    "## Micro Scores Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc69980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plt.groupby(['metric','method'])['value'].apply(nice_stat).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138ab12",
   "metadata": {},
   "source": [
    "# nice task names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55566a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "task_names = {\n",
    "#        'Diabetes mellitus with complications',\n",
    "       'Chronic obstructive pulmonary disease and bronchiectasis':'COPD and bronchiectasis',\n",
    "#        'Congestive heart failure nonhypertensive', \n",
    "#        'Conduction disorders',\n",
    "       'Hypertension with complications and secondary hypertension':'HTN with complications and secondary HTN',\n",
    "       'Diabetes mellitus without complication':'Diabetes mellitus', \n",
    "       'Essential hypertension':'Essential HTN',\n",
    "#        'Cardiac dysrhythmias', \n",
    "       'Chronic kidney disease':'CKD',\n",
    "       'Coronary atherosclerosis and other heart disease':'Heart disease',\n",
    "#        'Disorders of lipid metabolism', \n",
    "#        'Gastrointestinal hemorrhage', \n",
    "#        'Shock',\n",
    "#        'Pleurisy pneumothorax pulmonary collapse',\n",
    "#        'Acute cerebrovascular disease', \n",
    "#        'Other liver diseases',\n",
    "#        'Other lower respiratory disease',\n",
    "#        'Fluid and electrolyte disorders',\n",
    "#        'Acute myocardial infarction',\n",
    "#        'Other upper respiratory disease',\n",
    "#        'Acute and unspecified renal failure',\n",
    "       'Pneumonia (except that caused by tuberculosis or sexually transmitted disease)':'Pneumonia',\n",
    "       'Septicemia (except in labor)':'Septicemia',\n",
    "       'Respiratory failure insufficiency arrest (adult)':'Respiratory failure insufficiency arrest',\n",
    "       'Complications of surgical procedures or medical care':'Complications, surgical or medical'\n",
    "}\n",
    "nice_task=defaultdict(lambda x: x)\n",
    "nice_task.update(task_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bc56d",
   "metadata": {},
   "source": [
    "## combined data frame of all boostrapped results \n",
    "- number of nodes are also included, without bootstrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12333e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_lr.method.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61737a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.concat([df_lsr,df_feat_lr])\n",
    "\n",
    "# metrics = ['roc_auc_score','average_precision_score','ballogloss','log_loss']\n",
    "sort_metric = 'roc_auc_score'\n",
    "lstm = df_lsr.loc[df_lsr['metric']==sort_metric].groupby('task')['value'].mean() \n",
    "tmp = df_feat_lr.loc[df_feat_lr['metric']==sort_metric].groupby('task')['value'].mean()\n",
    "## task_idx\n",
    "task_idx = (np.abs(lstm - tmp)/lstm).sort_values().index\n",
    "# task_idx = (np.abs(lstm - tmp)/lstm).sort_values().index\n",
    "\n",
    "## Add n_nodes\n",
    "n_nodes1 = df_best.melt(id_vars=['run_id','method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "# feat_lr = pd.concat((df_best,n_nodes))\n",
    "\n",
    "df_best_lsr = df_lsr.groupby(['method','task','metric','n_nodes'])['value'].mean().reset_index()\n",
    "n_nodes2 = df_best_lsr.melt(id_vars=['method','task'],\n",
    "                            value_vars=['n_nodes'],\n",
    "                            var_name='metric',\n",
    "                            value_name='value')\n",
    "df_comb = pd.concat((df_comb,n_nodes1,n_nodes2))\n",
    "(df_comb\n",
    "  .loc[df_comb.metric.isin(metrics+['n_nodes'])]\n",
    "  .groupby(['task','method','metric'])\n",
    "  ['value']\n",
    "  .max()\n",
    "  .round(2)\n",
    "  .unstack()\n",
    "  .unstack()\n",
    " #  .loc[task_idx]\n",
    ")\n",
    "# df_comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8360de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadc99f",
   "metadata": {},
   "source": [
    "# catplot of performance and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b67364",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette=sns.color_palette(\"Paired\")\n",
    "# palette\n",
    "palette=palette[0:4]+[palette[7]]\n",
    "# print(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796fe86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style('whitegrid')\n",
    "mets = ['roc_auc_score', 'n_nodes']\n",
    "# mets = ['roc_auc_score']\n",
    "# mets = ['average_precision_score']\n",
    "# mets = ['n_nodes']\n",
    "\n",
    "df_plt = df_comb.loc[df_comb.metric.isin(mets)]\n",
    "\n",
    "df_plt['nice-task'] = df_plt['task'].apply(lambda x: task_names[x] if x in task_names.keys() else x)\n",
    "# task_order = df_plt[df_plt.metric=='roc_auc_score'].groupby('nice-task')['value'].mean().sort_values().index[::-1]\n",
    "task_order = df_plt[df_plt.metric==mets[0]].groupby('nice-task')['value'].mean().sort_values().index[::-1]\n",
    "print('task_order:',task_order)\n",
    "df_plt.loc[df_plt.method=='FEAT','method'] = 'FEAT-10'\n",
    "g = sns.catplot(\n",
    "    kind='point',\n",
    "    estimator=np.median,\n",
    "#     kind='strip',\n",
    "#     showfliers=False,\n",
    "#     dodge=False,\n",
    "    data=df_plt,\n",
    "    x='value',\n",
    "#     y='task',\n",
    "    y='nice-task',\n",
    "    hue='method',\n",
    "#     order=task_order,\n",
    "#     hue_order=['FEAT','FEAT-100','LR','LR-10','LR-100','LSTM'],\n",
    "    hue_order=['LR-10','FEAT-10','LR-100','FEAT-100','LSTM'],\n",
    "    col='metric',\n",
    "#     col_wrap=5,\n",
    "    ci='sd',\n",
    "#     scale=.7,\n",
    "    join=False,\n",
    "#     dodge=True,\n",
    "    height=6,\n",
    "#     aspect=2.4,\n",
    "    aspect=1.2,\n",
    "    sharex=False,\n",
    "#     palette='cividis_r',\n",
    "#     palette='nipy_spectral',\n",
    "#     palette='colorblind',\n",
    "#     palette='deep',\n",
    "    palette=palette\n",
    "#     palette='Paired'\n",
    "#     marker_colors = ['p','b','g','r'],\n",
    "#     row='metric'\n",
    ")\n",
    "g.set(ylabel='',xlabel='')\n",
    "# plt.xlabel('AUROC')\n",
    "\n",
    "for k,ax in g.axes_dict.items():\n",
    "    ax.grid(True,axis='y')\n",
    "    ax.grid(False,axis='x')\n",
    "    ax.set_title('')\n",
    "    if k=='roc_auc_score':\n",
    "        ax.set_title('AUROC')\n",
    "    elif k == 'average_precision_score':\n",
    "        ax.set_title('AUPRC')\n",
    "    elif k == 'n_nodes':\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_title('Model Size')\n",
    "        \n",
    "g.savefig('mimic3_detail-'+'-'.join(mets)+'-scores.pdf',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ae2ee",
   "metadata": {},
   "source": [
    "<!-- # table -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2156ed",
   "metadata": {},
   "source": [
    "# AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369aeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plt.loc[df_plt.metric=='roc_auc_score'].groupby(['nice-task','method'])['value'].apply(nice_stat).unstack().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5986c",
   "metadata": {},
   "source": [
    "# AUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb14456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plt.loc[df_plt.metric=='average_precision_score'].groupby(['nice-task','method'])['value'].apply(nice_stat).unstack().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4cd78",
   "metadata": {},
   "source": [
    "# Size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a30bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt.loc[df_plt.metric=='n_nodes'].groupby(['nice-task','method'])['value'].mean().unstack().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec4bfc",
   "metadata": {},
   "source": [
    "# Macro scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Annotator.configure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef06260",
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.configure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4432ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from statannotations.Annotator import Annotator\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style('whitegrid')\n",
    "# plt.figure(figsize=(5,9))\n",
    "# df_plt = pd.concat((df_best[['n_nodes','method','task']],\n",
    "#                    df_lsr[['n_nodes','method','task']]) # on = ['task'])\n",
    "#                   )\n",
    "# df_plt['nice-task'] = df_plt['task'].apply(lambda x: nice_task[x] if x in nice_task.keys() else x)\n",
    "n_nodes = df_best.melt(id_vars=['run_id','method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "# n_nodes['value'] = n_nodes['value']+10\n",
    "feat_lr = pd.concat((df_best.loc[df_best.fold=='test'],n_nodes))\n",
    "\n",
    "df_best_lsr = df_lsr.groupby(['method','task','metric','n_nodes'])['value'].mean().reset_index()\n",
    "n_nodes = df_best_lsr.melt(id_vars=['method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "lstm = pd.concat((df_best_lsr,n_nodes))\n",
    "\n",
    "df_plt = pd.concat((feat_lr,lstm))\n",
    "df_plt = df_plt.loc[df_plt.metric.isin(['roc_auc_score','average_precision_score','n_nodes'])]\n",
    "\n",
    "df_plt.loc[df_plt.method=='FEAT','method'] = '$FEAT_{10}$'\n",
    "df_plt['method'] = df_plt['method'].apply(lambda x: '$' +x.replace('-','_{') + '}$' if '-' in x else x)\n",
    "\n",
    "df_macro = df_plt\n",
    "\n",
    "order = ['$LR_{10}$','$FEAT_{10}$','$LR_{100}$','$FEAT_{100}$','LSTM']\n",
    "g = sns.catplot(\n",
    "    kind='box',\n",
    "    showfliers=False,\n",
    "    dodge=False,\n",
    "    data=df_plt,\n",
    "    y='value',\n",
    "    x='method',\n",
    "    order=order,\n",
    "    notch=True,\n",
    "    bootstrap=1000,\n",
    "    col='metric',\n",
    "    sharey=False,\n",
    "    legend=False,\n",
    "    color='w',\n",
    ")\n",
    "g.set(ylabel='',xlabel='')\n",
    "# plt.xlabel('AUROC')\n",
    "\n",
    "pairs=[('$LR_{10}$','$FEAT_{10}$'),\n",
    "       ('$LR_{100}$','$FEAT_{10}$'),\n",
    "       ('$LR_{100}$','$FEAT_{100}$'),\n",
    "       ('$FEAT_{100}$','LSTM')]\n",
    "for i,(k,ax) in enumerate(g.axes_dict.items()):\n",
    "    ax.grid(True,axis='y')\n",
    "    ax.grid(False,axis='x')\n",
    "    ttl = ax.get_title()[8:]\n",
    "    \n",
    "    ax.set_title('')\n",
    "    if k == 'roc_auc_score':\n",
    "        ax.set_ylabel('Macro AUROC')\n",
    "    elif k == 'average_precision_score':\n",
    "        ax.set_ylabel('Macro AUPRC')\n",
    "    elif k == 'n_nodes':\n",
    "#         ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "#         ax.set_xlabel('Model Size')\n",
    "        ax.set_ylabel('Model Size')\n",
    "#     ax.set(edgecolor='k') #,patch_edgecolor='k')\n",
    "    # make box edges black\n",
    "    for j, child in enumerate(ax._children):\n",
    "# #         print(j,box)\n",
    "        if hasattr(child,'_edgecolor'):\n",
    "            child.set_edgecolor('black')\n",
    "    for j, line in enumerate(ax.lines):\n",
    "        line.set_color('k')\n",
    "#         # iterate over whiskers and median lines\n",
    "# #         for k in range(6*j,6*(j+1)):\n",
    "# #          box.lines[k].set_color('black') \n",
    "#         box.set_color('black')\n",
    "    df_ax = df_plt.loc[df_plt.metric==k] \n",
    "    annotator = Annotator(ax, pairs, data=df_ax, x='method', y='value', order=order)\n",
    "    annotator.configure(\n",
    "#                         test='Mann-Whitney',\n",
    "                        test='Wilcoxon',\n",
    "#                         test='Kruskal',\n",
    "#                         test='t-test_paired',\n",
    "                        text_format='star',\n",
    "                        show_test_name=False, \n",
    "                        loc='inside',\n",
    "#                         comparisons_correction='holm'\n",
    "                        comparisons_correction='bonferroni'\n",
    "                       )\n",
    "    annotator.apply_and_annotate()\n",
    "    \n",
    "\n",
    "g.savefig('mimic3_macro-scores.pdf',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917192a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from statannotations.Annotator import Annotator\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style('whitegrid')\n",
    "# plt.figure(figsize=(5,9))\n",
    "# df_plt = pd.concat((df_best[['n_nodes','method','task']],\n",
    "#                    df_lsr[['n_nodes','method','task']]) # on = ['task'])\n",
    "#                   )\n",
    "# df_plt['nice-task'] = df_plt['task'].apply(lambda x: nice_task[x] if x in nice_task.keys() else x)\n",
    "n_nodes = df_best.melt(id_vars=['run_id','method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "# n_nodes['value'] = n_nodes['value']+10\n",
    "feat_lr = pd.concat((df_best.loc[df_best.fold=='test'],n_nodes))\n",
    "\n",
    "df_best_lsr = df_lsr.groupby(['method','task','metric','n_nodes'])['value'].mean().reset_index()\n",
    "n_nodes = df_best_lsr.melt(id_vars=['method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "lstm = pd.concat((df_best_lsr,n_nodes))\n",
    "\n",
    "df_plt = pd.concat((feat_lr,lstm))\n",
    "df_plt = df_plt.loc[df_plt.metric.isin(['roc_auc_score','average_precision_score','n_nodes'])]\n",
    "\n",
    "df_plt.loc[df_plt.method=='FEAT','method'] = '$FEAT_{10}$'\n",
    "df_plt['method'] = df_plt['method'].apply(lambda x: '$' +x.replace('-','_{') + '}$' if '-' in x else x)\n",
    "\n",
    "df_macro = df_plt\n",
    "\n",
    "order = ['$LR_{10}$','$FEAT_{10}$','$LR_{100}$','$FEAT_{100}$','LSTM']\n",
    "g = sns.catplot(\n",
    "    kind='strip',\n",
    "#     showfliers=False,\n",
    "#     dodge=False,\n",
    "    data=df_plt,\n",
    "    y='value',\n",
    "    x='method',\n",
    "    order=order,\n",
    "    col='metric',\n",
    "    sharey=False,\n",
    "    legend=False,\n",
    "    hue='task',\n",
    "    palette='cividis'\n",
    ")\n",
    "g.set(ylabel='',xlabel='')\n",
    "# plt.xlabel('AUROC')\n",
    "\n",
    "pairs=[('$LR_{10}$','$FEAT_{10}$'),\n",
    "       ('$LR_{100}$','$FEAT_{10}$'),\n",
    "       ('$LR_{100}$','$FEAT_{100}$'),\n",
    "       ('$FEAT_{100}$','LSTM')]\n",
    "for i,(k,ax) in enumerate(g.axes_dict.items()):\n",
    "    ax.grid(True,axis='y')\n",
    "    ax.grid(False,axis='x')\n",
    "    ttl = ax.get_title()[8:]\n",
    "    \n",
    "    ax.set_title('')\n",
    "    if k == 'roc_auc_score':\n",
    "        ax.set_ylabel('Macro AUROC')\n",
    "    elif k == 'average_precision_score':\n",
    "        ax.set_ylabel('Macro AUPRC')\n",
    "    elif k == 'n_nodes':\n",
    "#         ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "#         ax.set_xlabel('Model Size')\n",
    "        ax.set_ylabel('Model Size')\n",
    "#     ax.set(edgecolor='k') #,patch_edgecolor='k')\n",
    "    # make box edges black\n",
    "    for j, child in enumerate(ax._children):\n",
    "# #         print(j,box)\n",
    "        if hasattr(child,'_edgecolor'):\n",
    "            child.set_edgecolor('black')\n",
    "    for j, line in enumerate(ax.lines):\n",
    "        line.set_color('k')\n",
    "#         # iterate over whiskers and median lines\n",
    "# #         for k in range(6*j,6*(j+1)):\n",
    "# #          box.lines[k].set_color('black') \n",
    "#         box.set_color('black')\n",
    "    df_ax = df_plt.loc[df_plt.metric==k] \n",
    "    annotator = Annotator(ax, pairs, data=df_ax, x='method', y='value', order=order)\n",
    "    annotator.configure(\n",
    "#                         test='Mann-Whitney',\n",
    "                        test='Wilcoxon',\n",
    "#                         test='Kruskal',\n",
    "#                         test='t-test_paired',\n",
    "                        text_format='star',\n",
    "                        show_test_name=False, \n",
    "                        loc='inside',\n",
    "#                         comparisons_correction='holm'\n",
    "                        comparisons_correction='bonferroni'\n",
    "                       )\n",
    "    annotator.apply_and_annotate()\n",
    "    \n",
    "\n",
    "g.savefig('mimic3_macro-scores-strip.pdf',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbb6df",
   "metadata": {},
   "source": [
    "# percent differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a50df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "df_macro_lstm = df_macro.loc[df_macro.method=='LSTM'].set_index(['task','metric'])['value']\n",
    "df_macro_lstm\n",
    "frames = []\n",
    "for method, dfg in df_macro.groupby('method'):\n",
    "    d = pd.DataFrame((dfg.set_index(['task','metric'])['value'] - df_macro_lstm)/df_macro_lstm*100).reset_index()\n",
    "#     pdb.set_trace()\n",
    "    d['method'] = method\n",
    "    frames.append(d)\n",
    "# # df_lstm_diff = df_macro.set_index(['task','metric'])['value'] - df_macro.loc[df_macro.method=='lstm'].set_index(['task','metric'])['value']\n",
    "# # df_lstm_diff\n",
    "df_lstm_diff = pd.concat(frames)\n",
    "df_lstm_diff = df_lstm_diff.rename(columns = {'value':'% difference from LSTM'})\n",
    "order = ['$LR_{10}$','$FEAT_{10}$','$LR_{100}$','$FEAT_{100}$','LSTM']\n",
    "g = sns.catplot(\n",
    "    kind='box',\n",
    "#     showfliers=False,\n",
    "#     dodge=False,\n",
    "    data=df_lstm_diff,\n",
    "    y='% difference from LSTM',\n",
    "    x='method',\n",
    "    order=order,\n",
    "    col='metric',\n",
    "    col_order=['roc_auc_score','average_precision_score','n_nodes'],\n",
    "    sharey=False,\n",
    "    legend=False,\n",
    "#     hue='task',\n",
    "    palette='cividis'\n",
    ")\n",
    "# g.set(ylabel='% difference from LSTM')\n",
    "df_lstm_diff.groupby(['metric','method'])['% difference from LSTM'].mean().unstack().round(1)[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8120f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from statannotations.Annotator import Annotator\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style('whitegrid')\n",
    "# plt.figure(figsize=(5,9))\n",
    "# df_plt = pd.concat((df_best[['n_nodes','method','task']],\n",
    "#                    df_lsr[['n_nodes','method','task']]) # on = ['task'])\n",
    "#                   )\n",
    "# df_plt['nice-task'] = df_plt['task'].apply(lambda x: nice_task[x] if x in nice_task.keys() else x)\n",
    "n_nodes = df_best.melt(id_vars=['run_id','method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "# n_nodes['value'] = n_nodes['value']+10\n",
    "feat_lr = pd.concat((df_best.loc[df_best.fold=='test'],n_nodes))\n",
    "\n",
    "df_best_lsr = df_lsr.groupby(['method','task','metric','n_nodes'])['value'].mean().reset_index()\n",
    "n_nodes = df_best_lsr.melt(id_vars=['method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "lstm = pd.concat((df_best_lsr,n_nodes))\n",
    "\n",
    "df_plt = pd.concat((feat_lr,lstm))\n",
    "df_plt = df_plt.loc[df_plt.metric.isin(['roc_auc_score','average_precision_score','n_nodes'])]\n",
    "\n",
    "df_plt.loc[df_plt.method=='FEAT','method'] = '$FEAT_{10}$'\n",
    "df_plt['method'] = df_plt['method'].apply(lambda x: '$' +x.replace('-','_{') + '}$' if '-' in x else x)\n",
    "\n",
    "df_macro = df_plt\n",
    "\n",
    "order = ['$LR_{10}$','$FEAT_{10}$','$LR_{100}$','$FEAT_{100}$','LSTM']\n",
    "g = sns.catplot(\n",
    "    kind='box',\n",
    "    showfliers=False,\n",
    "    dodge=False,\n",
    "    data=df_plt,\n",
    "    y='value',\n",
    "    x='method',\n",
    "    order=order,\n",
    "    notch=True,\n",
    "    bootstrap=1000,\n",
    "    col='metric',\n",
    "    sharey=False,\n",
    "    legend=False,\n",
    "    color='w',\n",
    ")\n",
    "g.set(ylabel='',xlabel='')\n",
    "# plt.xlabel('AUROC')\n",
    "\n",
    "pairs=[('$LR_{10}$','$FEAT_{10}$'),\n",
    "       ('$LR_{100}$','$FEAT_{10}$'),\n",
    "       ('$LR_{100}$','$FEAT_{100}$'),\n",
    "       ('$FEAT_{100}$','LSTM')]\n",
    "for i,(k,ax) in enumerate(g.axes_dict.items()):\n",
    "    ax.grid(True,axis='y')\n",
    "    ax.grid(False,axis='x')\n",
    "    ttl = ax.get_title()[8:]\n",
    "    \n",
    "    ax.set_title('')\n",
    "    if k == 'roc_auc_score':\n",
    "        ax.set_ylabel('Macro AUROC')\n",
    "    elif k == 'average_precision_score':\n",
    "        ax.set_ylabel('Macro AUPRC')\n",
    "    elif k == 'n_nodes':\n",
    "#         ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "#         ax.set_xlabel('Model Size')\n",
    "        ax.set_ylabel('Model Size')\n",
    "#     ax.set(edgecolor='k') #,patch_edgecolor='k')\n",
    "    # make box edges black\n",
    "    for j, child in enumerate(ax._children):\n",
    "# #         print(j,box)\n",
    "        if hasattr(child,'_edgecolor'):\n",
    "            child.set_edgecolor('black')\n",
    "    for j, line in enumerate(ax.lines):\n",
    "        line.set_color('k')\n",
    "#         # iterate over whiskers and median lines\n",
    "# #         for k in range(6*j,6*(j+1)):\n",
    "# #          box.lines[k].set_color('black') \n",
    "#         box.set_color('black')\n",
    "    df_ax = df_plt.loc[df_plt.metric==k] \n",
    "    annotator = Annotator(ax, pairs, data=df_ax, x='method', y='value', order=order)\n",
    "    annotator.configure(\n",
    "#                         test='Mann-Whitney',\n",
    "                        test='Wilcoxon',\n",
    "#                         test='Kruskal',\n",
    "#                         test='t-test_paired',\n",
    "                        text_format='star',\n",
    "                        show_test_name=False, \n",
    "                        loc='inside',\n",
    "#                         comparisons_correction='holm'\n",
    "                        comparisons_correction='bonferroni'\n",
    "                       )\n",
    "    annotator.apply_and_annotate()\n",
    "    \n",
    "\n",
    "g.savefig('mimic3_macro-scores.pdf',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c63b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt.loc[(df_plt.method=='$FEAT_{10}$') & (df_plt.metric=='roc_auc_score')] #.groupby(['method','metric'])['value']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef0c52",
   "metadata": {},
   "source": [
    "# macro table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1163f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_macro.groupby(['metric','method'])['value'].apply(nice_stat).unstack()\n",
    "# tmp.loc[['roc_auc_score','average_precision_score','n_nodes']][order].transpose()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d6501",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- look at micro AUROC/AUPRC\n",
    "- first or second figure? \n",
    "- selecting from archive\n",
    "- different model selection for FEAT / LR\n",
    "    - do something conventional/standard for LR\n",
    "- push changes to main repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b58ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.loc[df_best.method=='FEAT',:].groupby('task')['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6417184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.loc[df_best.method=='FEAT-100',:].groupby('task')['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24196957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_lsr = df_lsr.groupby(['method','task','metric','n_nodes'])['value'].mean().reset_index()\n",
    "n_nodes = df_best_lsr.melt(id_vars=['method','task'],value_vars=['n_nodes'],var_name='metric',value_name='value')\n",
    "pd.concat((df_best_lsr,n_nodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
